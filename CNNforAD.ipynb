# Step 1: Import required libraries
import torch
import torch.nn as nn
import torch.nn.functional as F

print("‚úÖ Libraries imported successfully!")


# Step 2: Define the CNN model
class AnimalDetectionCNN(nn.Module):
    def __init__(self, num_classes=2):  # Default is 2 classes (e.g., 'animal', 'no animal')
        super(AnimalDetectionCNN, self).__init__()

        # First convolutional block
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(32)

        # Second convolutional block
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)

        # Third convolutional block
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(128)

        # Max pooling and dropout layers
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.dropout = nn.Dropout(0.3)

        # Fully connected layers
        # After 3 pooling layers: 224 -> 112 -> 56 -> 28
        self.fc1 = nn.Linear(128 * 28 * 28, 256)
        self.fc2 = nn.Linear(256, num_classes)

    def forward(self, x):
        x = self.pool(F.relu(self.bn1(self.conv1(x))))
        x = self.pool(F.relu(self.bn2(self.conv2(x))))
        x = self.pool(F.relu(self.bn3(self.conv3(x))))
        x = x.view(-1, 128 * 28 * 28)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

print("‚úÖ CNN class defined!")


import os

dataset_path = '/root/.cache/kagglehub/datasets/antoreepjana/animals-detection-images-dataset/versions/7/train'
class_names = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]
print("Detected classes:", class_names)

num_classes = len(class_names)
print("Number of classes:", num_classes)

model = AnimalDetectionCNN(num_classes=num_classes)


torch.save(model.state_dict(), 'animal_detection_cnn.pth')
print("‚úÖ Model weights saved to 'animal_detection_cnn.pth'")

import kagglehub

# Download latest version
path = kagglehub.dataset_download("antoreepjana/animals-detection-images-dataset")

print("Path to dataset files:", path)


import os

for root, dirs, files in os.walk(path):
    print(f"üìÅ {root}")
    for f in files[:5]:  # preview 5 files per folder
        print("  ‚îî‚îÄ‚îÄ", f)


import cv2
import matplotlib.pyplot as plt

img_path = os.path.join(img_dir, img_files[0])
label_path = os.path.join(label_dir, os.path.splitext(img_files[0])[0] + ".txt")

img = cv2.imread(img_path)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
h, w, _ = img.shape

with open(label_path, 'r') as f:
    for line in f:
        cls, x, y, bw, bh = map(float, line.strip().split())
        x1 = int((x - bw / 2) * w)
        y1 = int((y - bh / 2) * h)
        x2 = int((x + bw / 2) * w)
        y2 = int((y + bh / 2) * h)
        cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)

plt.imshow(img)
plt.axis('off')
plt.title("Image + YOLO Bounding Box")
plt.show()


import os

# üêÆ STEP 1: Define path to main animal folder
main_dir = '/root/.cache/kagglehub/datasets/antoreepjana/animals-detection-images-dataset/versions/7/train'

# üêµ STEP 2: Automatically detect class names from folder structure
class_names = [name for name in os.listdir(main_dir) if os.path.isdir(os.path.join(main_dir, name))]

# üÜî Create class-to-id mapping
class_to_id = {name: idx for idx, name in enumerate(sorted(class_names))}
print("üìö Class to ID mapping:", class_to_id)

# üßπ STEP 3: Loop over each class folder and fix labels
for class_name in class_names:
    label_folder = os.path.join(main_dir, class_name, 'Label')

    if not os.path.exists(label_folder):
        print(f"‚ö†Ô∏è No 'Label' folder for class {class_name}")
        continue

    for label_file in os.listdir(label_folder):
        if label_file.endswith('.txt'):
            label_path = os.path.join(label_folder, label_file)

            with open(label_path, 'r') as f:
                lines = f.readlines()

            new_lines = []
            for line in lines:
                parts = line.strip().split()
                if len(parts) == 5 and parts[0] in class_to_id:
                    class_id = class_to_id[parts[0]]
                    new_line = f"{class_id} {' '.join(parts[1:])}\n"
                    new_lines.append(new_line)
                else:
                    print(f"‚ùå Skipping malformed line in {label_file}: {line.strip()}")

            with open(label_path, 'w') as f:
                f.writelines(new_lines)

print("‚úÖ Conversion complete! All label files now use YOLO class IDs.")


# Save class names for data.yaml
with open('classes.txt', 'w') as f:
    for name in sorted(class_names):
        f.write(f"{name}\n")
print("üìÑ Saved 'classes.txt' with class list.")


#TRAINING 

from torchvision import datasets, transforms
from torch.utils.data import DataLoader

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

train_root = '/root/.cache/kagglehub/datasets/antoreepjana/animals-detection-images-dataset/versions/7/train'

train_dataset = datasets.ImageFolder(root=train_root, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

print("‚úÖ train_loader ready with", len(train_dataset), "images and", len(train_loader), "batches")
print("Classes found:", train_dataset.classes)

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# Dataset directory
train_root = '/root/.cache/kagglehub/datasets/antoreepjana/animals-detection-images-dataset/versions/7/train'

# Resize and convert images to tensor
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

# Load training data with labels based on subfolders
train_dataset = datasets.ImageFolder(root=train_root, transform=transform)

# Create batches of size 32 and shuffle the data
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# Print summary
print(f"Classes found: {train_dataset.classes}")
print(f"Total training images: {len(train_dataset)}")




for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0

    print(f"\nüöÄ Starting Epoch {epoch+1}/{num_epochs}...")

    for batch_idx, (images, labels) in enumerate(train_loader):
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

        # Print progress every 10 batches
        if (batch_idx + 1) % 10 == 0 or (batch_idx + 1) == len(train_loader):
            print(f"  üß™ Batch {batch_idx+1}/{len(train_loader)} | Loss: {loss.item():.4f}")

    avg_loss = running_loss / len(train_loader)
    print(f"‚úÖ Epoch [{epoch+1}/{num_epochs}] complete | Avg Loss: {avg_loss:.4f}")




def validate(model, data_loader, criterion, device):
    model.eval()  # Set model to evaluation mode
    total_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():  # No gradient computation during validation
        for inputs, labels in data_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)
            total_loss += loss.item()

            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    avg_loss = total_loss / len(data_loader)
    accuracy = 100 * correct / total
    return avg_loss, accuracy
